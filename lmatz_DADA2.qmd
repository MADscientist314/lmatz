---
title: "DADA2 Processing Script for Type 2 Diabetes association"
author: "Jochum, Michael D"
format: 
  html:
    theme: sandstone
editor: visual
---

## Objective

The purpose of the this script is to conduct the following:

1.  Data Import - take the trimmed fastq files
2.  Data Wrangling / QA / Filter and trimming
3.  DADA2
    1.  Error rate estimation
    2.  Dereplication
    3.  ASV Calling
    4.  Read pair merging
    5.  Chimera removal
4.  DECIPHER - Taxonomic Classification
5.  Data Export
    1.  ASV counts
    2.  Taxonomy matrix
    3.  ASV Fasta

------------------------------------------------------------------------

### 1. Data Import

Lets start by loading the libraries, setting the working directory, and importing the fastq files output from trimmomatic

```{r}
library(dada2)
library(tidyverse)
library(knitr)
library(DECIPHER)

setwd("/home/jochum00/lmatz/LM_RawSequences/paired")
######################################
### DATA WRANGLING
#import the dataframe with the manifest,tsv and make vectors of the fwd and rev
#######################################

df<-read.table("../manifest.tsv",header = F,sep = "\t",row.names = NULL)
colnames(df)<-"samples"
df<-df%>%mutate(fwd=paste0(samples,".1.paired.fq"),
                rev=paste0(samples,".2.paired.fq"),
                filt_fwd=paste0(samples,".1.paired.filt.fq"),
                filt_rev=paste0(samples,".2.paired.filt.fq"),
                )

```

------------------------------------------------------------------------

### 2. Data Wrangling / QA / Filter and trimming

Make a vector containing the forward, reverse, filtered forward, and filtered reverse read name

```{r}
# one holding the file names of all the forward reads
forward_reads <- df$fwd
# and one with the reverse
reverse_reads <- df$rev
# and variables holding file names for the forward and reverse
# filtered reads we're going to generate below
filtered_forward_reads <-df$filt_fwd
filtered_reverse_reads <- df$filt_rev
```

Lets take a look at the quality profiles of the trimmed reads for the first 5 samples

```{r}
plotQualityProfile(reverse_reads[1:5])
plotQualityProfile(forward_reads[1:5])

```

Ok looks like the read quality drops off around 200bp for the reverse, so lets truncate reverse reads starting at 200bp for quality thresholds less than Q20, and lets remove reads that are less than 50bp in length.

```{r}
filtered_out <- filterAndTrim(forward_reads, 
                              filtered_forward_reads,
                              reverse_reads, 
                              filtered_reverse_reads, 
                              maxEE=c(2,2),
                              rm.phix=TRUE, 
                              multithread = T,
                              minLen=50,
                              truncLen=c(250,200))
```

Ok lets take a look at what that did to our dataset

```{r}
class(filtered_out) # matrix
dim(filtered_out) # 20 2
kable(filtered_out)
#           
# reads.in reads.out
# Buffington_201_001.1.paired.fq    17878     15287
# Buffington_201_002.1.paired.fq    19088     17182
# Buffington_201_003.1.paired.fq    18775     16559
# Buffington_201_005.1.paired.fq    22986     19944
# Buffington_201_006.1.paired.fq    19611     17239
# Buffington_201_007.1.paired.fq    23630     20788
# Buffington_201_008.1.paired.fq    19337     17473
# Buffington_201_009.1.paired.fq    13076     11231
# Buffington_201_010.1.paired.fq    22531     19794
# Buffington_201_011.1.paired.fq    19323     15999
# Buffington_201_013.1.paired.fq    17213     15100
# Buffington_201_014.1.paired.fq    21178     17680
```

not too bad, we didn't loose too many reads so lets keep going.

------------------------------------------------------------------------

### 3. DADA2

3.1 Error rate estimation

Learn the error rates

```{r}
err_forward_reads <- learnErrors(filtered_forward_reads, multithread=TRUE) 
# 51069000 total bases in 204276 reads from 12 samples will be used for learning the error rates.
err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread=TRUE) 
# 40855200 total bases in 204276 reads from 12 samples will be used for learning the error rates.
```

Plot the learned error rates

```{r}
plotErrors(err_forward_reads, nominalQ=TRUE)
plotErrors(err_reverse_reads, nominalQ=TRUE)
```

#### 3.2 Dereplication

Dereplicate the forward and reverse reads and give them identical names for matching

```{r}
derep_forward <- derepFastq(filtered_forward_reads, verbose=TRUE)
names(derep_forward) <-df$samples # the sample names in these objects are initially the file names of the samples, this sets them to the sample names for the rest of the workflow
derep_reverse <- derepFastq(filtered_reverse_reads, verbose=TRUE)
names(derep_reverse) <- df$samples
```

#### 3.3 ASV Calling

Run the DADA2 algorithm to generate the ASVs

```{r}
#run DADA2 
dada_forward <- dada(derep_forward, err=err_forward_reads, pool=T, multithread=TRUE)
# 12 samples were pooled: 204276 reads in 46370 unique sequences.
dada_reverse <- dada(derep_reverse, err=err_reverse_reads, pool=T, multithread=TRUE)
# 12 samples were pooled: 204276 reads in 44362 unique sequences.
```

#### 3.4 Read pair merging

Merging forward and reverse reads

```{r}
#Merging forward and reverse reads
merged_amplicons <- mergePairs(dada_forward, derep_forward, dada_reverse,
                               derep_reverse, trimOverhang=TRUE, minOverlap=150)
# Generate a count table
```

Generate a count table

```{r}
seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab) # matrix
dim(seqtab) #  12 729
```

#### 3.5 Chimera removal

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, verbose=T) 
# Identified 195 bimeras out of 729 input sequences.
# though we only lost 17 sequences, we don't know if they held a lot in terms of abundance, this is one quick way to look at that
sum(seqtab.nochim)/sum(seqtab) 
#  0.9485947
```

Generate an overview of the ASV counts throughout all the samples

```{r}
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=df$samples, 
                          dada2_input=filtered_out[,1],
                          filtered=filtered_out[,2], 
                          dada_f=sapply(dada_forward, getN),
                          dada_r=sapply(dada_reverse, getN),
                          merged=sapply(merged_amplicons, getN),
                          nonchim=rowSums(seqtab.nochim),
                          final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))
kable(summary_tab)
```

Lets write the summary table our to a file for reference

```{r}
write.table(summary_tab, "read-count-tracking.tsv", quote=FALSE, sep="\t", col.names=NA)
```

------------------------------------------------------------------------

### 4. DECIPHER - Taxonomic Classification

Lets start by downloading the silva138 reference database (if you dont already have it) and loading it into our environment

```{r}
# download the reference taxonomy data
## downloading DECIPHER-formatted SILVA v138 reference
download.file(url="http://www2.decipher.codes/Classification/TrainingSets/SILVA_SSU_r138_2019.RData", destfile="SILVA_SSU_r138_2019.RData")

## loading reference taxonomy object
load("SILVA_SSU_r138_2019.RData")
## creating DNAStringSet object of our ASVs
dna <- DNAStringSet(getSequences(seqtab.nochim))
## and classifying
tax_info <- IdTaxa(test=dna, trainingSet=trainingSet, strand="both", processors=48)
################################################################################
```

Now lets make a DNAStringSet object using the nonchimeric sequences that we have generated from our ASVs

```{r}
## creating DNAStringSet object of our ASVs
dna <- DNAStringSet(getSequences(seqtab.nochim))
```

And finally lets taxonomically classify our ASVs based on the Silva db using DECIPHER

*\*\*\* NOTE: FOR Parallel processing, you need to change the number "cores" to the number of processors that the computer running the analysis has \*\*\**

```{r}
## CHANGE THIS NUMBER TO THE AMOUNT OF THREADS THAT YOUR MACHINE HAS
cores<-48
#####
tax_info <- IdTaxa(test=dna, trainingSet=trainingSet, strand="both", processors=cores)
################################################################################
```

### 5. Data Export

Lets make our seqence headers have more manageable names (ASV_1, ASV_2...)

```{r}
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}
```

### 5.1 ASV counts

write out the count matrix to a file

```{r}
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, "ASVs_counts.tsv", sep="\t", quote=F, col.names=NA)

```

### 5.2 Taxonomy

Make rank names for the taxonomy data, wrangle out the \> sign for the ASV headers, and export the taxonomy data

```{r}
ranks <- c("domain", "phylum", "class", "order", "family", "genus", "species")
asv_tax <- t(sapply(tax_info, function(x) {
  m <- match(ranks, x$rank)
  taxa <- x$taxon[m]
  taxa[startsWith(taxa, "unclassified_")] <- NA
  taxa
}))
colnames(asv_tax) <- ranks
rownames(asv_tax) <- gsub(pattern=">", replacement="", x=asv_headers)

write.table(asv_tax, "ASVs_taxonomy.tsv", sep = "\t", quote=F, col.names=NA)

```

### 5.3 ASV Fasta

Lets write out our fasta file containing our final ASV seqs:

```{r}
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "ASVs.fa")

```

------------------------------------------------------------------------

// End of File
